{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ROOT import (TCanvas, TPad, TFile, TPaveLabel, \n",
    "                  TPaveText, gROOT, TH1F, TH1D, TLegend, \n",
    "                  gStyle, TH2F, TChain, TGraphErrors, TText, gPad, gROOT, TTree)\n",
    "from array import array\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "        \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, SimpleRNN, GRU, Masking, Lambda, Reshape, Dropout, RNN\n",
    "from keras.optimizers import Adagrad, SGD, RMSprop, Adam\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, LSTM, Conv1D, SimpleRNN, Concatenate\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import keras.backend as K\n",
    "import pickle\n",
    "#import seaborn as sns\n",
    "\n",
    "TODAY = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def init_dirs():\n",
    "    import datetime, os\n",
    "    for _dir in [\"plots\", \"models\"]:\n",
    "        today_dir = os.path.join(_dir, TODAY)\n",
    "        if not os.path.isdir(today_dir):\n",
    "            os.makedirs(today_dir)\n",
    "            \n",
    "#init_dirs()\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 23 features\n",
      "mZp is MZ2\n"
     ]
    }
   ],
   "source": [
    "# all methods here\n",
    "def clean_arrays(data, weights=None):\n",
    "    new = []\n",
    "    for d in data:\n",
    "        #if all(di == 0. for di in d):\n",
    "        if d[-1]==0.:\n",
    "            continue\n",
    "        new.append(d)\n",
    "    out = np.array(new)\n",
    "    print(\"before\", data.shape, 'after', out.shape)\n",
    "    return out\n",
    "\n",
    "def build_array(fname, features, cuts, mll_idx = 10, should_clean_arrays=True, **kwargs):\n",
    "    #print(features[mll_idx])\n",
    "    f = TFile.Open(fname)\n",
    "    tree = f.tree_MUON_SCALE__1down\n",
    "    n_events = tree.GetEntries()\n",
    "    data = np.zeros((n_events, len(features)))\n",
    "    for n, (event, element) in enumerate(zip(tree, data)):\n",
    "        for i, feature in enumerate(features):\n",
    "            try:\n",
    "                val = getattr(event, feature)\n",
    "            except:\n",
    "                val = None\n",
    "            if feature in cuts and eval(\"{}{}\".format(val, cuts[feature])):\n",
    "                element[i] = getattr(event, feature)\n",
    "            elif feature in cuts:\n",
    "                element = np.zeros(len(features))\n",
    "                break\n",
    "            elif feature == 'train_mass':\n",
    "                element[i] = getattr(event, features[mll_idx])\n",
    "            elif feature == 'MZ1_MZ2':\n",
    "                element[i] = getattr(event, 'MZ1')-getattr(event, 'MZ2')\n",
    "            else:\n",
    "                element[i] = val\n",
    "    return clean_arrays(data) if should_clean_arrays else data\n",
    "\n",
    "features = ('train_mass', ## 0\n",
    "            'PtL1',       ## 1\n",
    "            'PtL2',       ## 2\n",
    "            'PtL3',       ## 3\n",
    "            'PtL4',       ## 4\n",
    "            'EtaL1',      ## 5\n",
    "            'EtaL2',      ## 6\n",
    "            'EtaL3',      ## 7\n",
    "            'EtaL4',      ## 8\n",
    "            'MZ1',        ## 9\n",
    "            'MZ2',        ## 10\n",
    "            'MZ1_MZ2',    ## 11\n",
    "            'PtZ1',       ## 12\n",
    "            'PtZ2',       ## 13\n",
    "            'MZZ',        ## 14\n",
    "            'PtZZ',       ## 15\n",
    "            'DeltaRl12',  ## 16\n",
    "            'DeltaRl34',  ## 17\n",
    "            'dEtal12',    ## 18\n",
    "            'dEtal34',    ## 19\n",
    "            'run',        ## 20\n",
    "            'event',      ## 21\n",
    "            'weight')     ## 22\n",
    "\n",
    "# these are your cuts that preselect events, empty now\n",
    "cuts = dict()\n",
    "#cuts = {\"MZZ\": \"<120\"}\n",
    "\n",
    "def unison_shuffled_copies(*arr):\n",
    "    assert all(len(a) for a in arr)\n",
    "    p = np.random.permutation(len(arr[0]))\n",
    "    return (a[p] for a in arr)\n",
    "\n",
    "mll_idx = 10 # 10 when signal sample < 40GeV; 9 when signal >40GeV\n",
    "\n",
    "print(\"have {} features\".format(len(features)))\n",
    "print(\"mZp is\", features[mll_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over samples and create skimmed files only containing interesting features\n",
    "\n",
    "Make sure all directories are correct. In this case I save my train and test data to ``example_hep/train`` and ``example_hep/test``. I only use the files in ``train`` for training and for evaluation I only use ``test``.\n",
    "\n",
    "The msamp list contains the sample number and masses.\n",
    "\n",
    "## So first for the signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (2951, 23) after (2950, 23)\n",
      "Training, validation, testing:  (1770, 23) (590, 23) (590, 23)\n",
      "before (9781, 23) after (9773, 23)\n",
      "Training, validation, testing:  (5863, 23) (1955, 23) (1955, 23)\n",
      "before (8610, 23) after (8602, 23)\n",
      "Training, validation, testing:  (5161, 23) (1720, 23) (1721, 23)\n",
      "before (8316, 23) after (8311, 23)\n",
      "Training, validation, testing:  (4986, 23) (1662, 23) (1663, 23)\n",
      "before (7433, 23) after (7427, 23)\n",
      "Training, validation, testing:  (4456, 23) (1485, 23) (1486, 23)\n",
      "before (8702, 23) after (8697, 23)\n",
      "Training, validation, testing:  (5218, 23) (1739, 23) (1740, 23)\n",
      "before (6508, 23) after (6500, 23)\n",
      "Training, validation, testing:  (3900, 23) (1300, 23) (1300, 23)\n",
      "before (6982, 23) after (6975, 23)\n",
      "Training, validation, testing:  (4185, 23) (1395, 23) (1395, 23)\n",
      "before (7814, 23) after (7807, 23)\n",
      "Training, validation, testing:  (4684, 23) (1561, 23) (1562, 23)\n",
      "before (8423, 23) after (8414, 23)\n",
      "Training, validation, testing:  (5048, 23) (1683, 23) (1683, 23)\n",
      "before (4457, 23) after (4452, 23)\n",
      "Training, validation, testing:  (2671, 23) (890, 23) (891, 23)\n",
      "before (4514, 23) after (4508, 23)\n",
      "Training, validation, testing:  (2704, 23) (902, 23) (902, 23)\n",
      "before (4586, 23) after (4583, 23)\n",
      "Training, validation, testing:  (2749, 23) (917, 23) (917, 23)\n",
      "before (4502, 23) after (4499, 23)\n",
      "Training, validation, testing:  (2699, 23) (900, 23) (900, 23)\n",
      "before (4177, 23) after (4174, 23)\n",
      "Training, validation, testing:  (2504, 23) (835, 23) (835, 23)\n",
      "before (6860, 23) after (6852, 23)\n",
      "Training, validation, testing:  (4111, 23) (1370, 23) (1371, 23)\n",
      "before (9515, 23) after (9509, 23)\n",
      "Training, validation, testing:  (5705, 23) (1902, 23) (1902, 23)\n",
      "before (4756, 23) after (4749, 23)\n",
      "Training, validation, testing:  (2849, 23) (950, 23) (950, 23)\n",
      "before (3700, 23) after (3695, 23)\n",
      "Training, validation, testing:  (2217, 23) (739, 23) (739, 23)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "msamp = [5,7,9,11,13,15,17,19,23,27,31,35,39,45,51,57,63,69,75]\n",
    "\n",
    "## Need to do rescale to only training datasets\n",
    "## firstly sum them together to fit transform\n",
    "#raw_bkg1 = build_array(\n",
    "#    '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_bk.root',\n",
    "#    features, cuts, mll_idx=10)\n",
    "#raw_train = raw_bkg1[0:int(len(raw_bkg1)*0.6)]\n",
    "#raw_train[:,0] = 40\n",
    "#print (np.amax(raw_train, axis=0), np.amin(raw_train, axis=0))\n",
    "#for mass in msamp:\n",
    "#    if mass<40:\n",
    "#        m_idx=10\n",
    "#    else:\n",
    "#        m_idx=9\n",
    "#    raw_signal_add = build_array(\n",
    "#        '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_{}GeV.root'.format(mass), \n",
    "#        features, cuts, m_idx)\n",
    "#    raw_train_add = raw_signal_add[0:int(len(raw_signal_add)*0.6)]\n",
    "#    raw_train = np.concatenate((raw_train,raw_train_add))\n",
    "#print (np.amax(raw_train, axis=0), np.amin(raw_train, axis=0))\n",
    "\n",
    "#min_max_scaler.fit_transform(raw_train[:,0:20])\n",
    "\n",
    "for mass in msamp:\n",
    "    if mass<40:\n",
    "        m_idx=10\n",
    "    else:\n",
    "        m_idx=9\n",
    "    signal = build_array(\n",
    "        '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_{}GeV.root'.format(mass), \n",
    "        features, cuts, m_idx)\n",
    "    #signal = np.zeros( (len(raw_signal), 43))\n",
    "    #signal[:,0:20] = min_max_scaler.transform(raw_signal[:,0:20])\n",
    "    #signal[:,20:40] = raw_signal[:,0:20]\n",
    "    #signal[:,40:43] = raw_signal[:,20:23]\n",
    "    #print(raw_signal[0:3])\n",
    "    #print(signal[0:3])\n",
    "   \n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/data_npy/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, signal)\n",
    "\n",
    "    n_trains = int(len(signal)*0.6)\n",
    "    n_vals = int(len(signal)*0.8)\n",
    "    \n",
    "    strain = signal[0:n_trains]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/train/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, strain)\n",
    "    \n",
    "    sval = signal[n_trains:n_vals]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/Validation/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, sval)\n",
    "    \n",
    "    stest = signal[n_vals:]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/test/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, stest)\n",
    "        \n",
    "    print (\"Training, validation, testing: \", strain.shape, sval.shape, stest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now for the background samples\n",
    "Need to re-define the mass parameter (train_mass) for backgrounds used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (53000, 23) after (52954, 23)\n",
      "Training, validation, testing:  (31772, 23) (10591, 23) (10591, 23)\n"
     ]
    }
   ],
   "source": [
    "bkg1 = build_array(\n",
    "    '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_bk.root',\n",
    "    features, cuts, mll_idx=10)\n",
    "\n",
    "#print(raw_bkg1[0:3])\n",
    "#bkg1 = np.zeros( (len(raw_bkg1), 43))\n",
    "#bkg1[:,0:20] = min_max_scaler.transform(raw_bkg1[:,0:20])\n",
    "#bkg1[:,20:40] = raw_bkg1[:,0:20]\n",
    "#bkg1[:,40:43] = raw_bkg1[:,20:23]\n",
    "#print(bkg1[0:3])\n",
    "\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/data_npy/tree_bk.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, bkg1)\n",
    "\n",
    "n_trains = int(len(bkg1)*0.6)\n",
    "n_vals = int(len(bkg1)*0.8)\n",
    "\n",
    "btrain = bkg1[0:n_trains]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/train/tree_bkg.npy'.format(mass), 'wb') as f:\n",
    "    np.save(f, btrain)\n",
    "    \n",
    "bval = bkg1[n_trains:n_vals]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/Validation/tree_bkg.npy'.format(mass), 'wb') as f:\n",
    "    np.save(f, bval)\n",
    "    \n",
    "btest = bkg1[n_vals:]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/sysData_down/test/tree_bkg.npy'.format(mass), 'wb') as f:\n",
    "    np.save(f, btest)\n",
    "    \n",
    "print (\"Training, validation, testing: \", btrain.shape, bval.shape, btest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
